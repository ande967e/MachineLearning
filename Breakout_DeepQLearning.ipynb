{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Section in which we build our CNN - Brain\n",
    "\n",
    "# Importing the libraries\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "nRows = 105                  #Number of rows in the image.     210\n",
    "nColumns = 80                #Number of columns in the image.  160\n",
    "\n",
    "# Creating the Brain class\n",
    "class Brain():\n",
    "    def __init__(self, iS = (nRows,nColumns,3), lr = 0.0005):  # is = input shape    lr = learning rate\n",
    "        self.learningRate = lr \n",
    "        self.inputShape = iS\n",
    "        self.numOutputs = 3         # three actions total, left, right and nothing\n",
    "        self.model = Sequential()\n",
    "        # Adding layers to the model\n",
    "        self.model.add(Conv2D(32, (3,3), activation = 'relu', input_shape = self.inputShape))  #32 3x3 filters with the ReLU activation function. You\n",
    "        self.model.add(MaxPooling2D((2,2)))                                                    #adding a max pooling layer. The window's size is 2x2, which will shrink the feature maps in size by 2.\n",
    "        self.model.add(Conv2D(64, (2,2), activation = 'relu'))                                 #second convolution layer. This time it has 64 2x2 filters, with the same ReLU activation function.\n",
    "        self.model.add(Flatten())                                                              #flatten to a 1D vector. Here the 2D images is flattened to a 1D vector, which we then will be able to use as the input to your neural network.\n",
    "        self.model.add(Dense(units = 256, activation = 'relu'))                                #full connection step – building the traditional ANN. This specific line adds a new hidden layer with 256 neurons and the ReLU activation function to the model.\n",
    "        self.model.add(Dense(units = self.numOutputs)) #last layer of the neural network – the output layer. It has as many outputs as there are actions. By not mentioning an activation function, it defaults to a linear.\n",
    "        # Compiling the model\n",
    "        self.model.compile(loss = 'mean_squared_error', optimizer = Adam(lr = self.learningRate)) #how to calculate the error (indicated by loss), and which optimizer to use when training the model\n",
    "\n",
    "        # Making a function that will load a model from a file\n",
    "        def loadModel(self, filepath):\n",
    "            self.model = load_model(filepath)\n",
    "            return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section containing the environment (Breakout game) - Environment\n",
    "import gym\n",
    "import atari_py\n",
    "env = gym.make(\"Breakout-v4\")\n",
    "env.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section that builds the Experience Replay Memory - DQN\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# IMPLEMENTING DEEP Q-LEARNING WITH EXPERIENCE REPLAY\n",
    "\n",
    "class Dqn(object):\n",
    "    \n",
    "    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN\n",
    "    def __init__(self, max_memory = 100, discount = 0.9):\n",
    "        self.memory = list()\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "\n",
    "    # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY\n",
    "    def remember(self, transition, game_over):\n",
    "        self.memory.append([transition, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY\n",
    "    def get_batch(self, model, batch_size = 10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_outputs = model.output_shape[-1]\n",
    "        \n",
    "        # Input batch which works with 3D states\n",
    "        inputs = np.zeros((min(len_memory, batch_size), self.memory[0][0][0].shape[1],self.memory[0][0][0].shape[2],self.memory[0][0][0].shape[3]))\n",
    "        \n",
    "        targets = np.zeros((min(len_memory, batch_size), num_outputs))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):\n",
    "            current_state, action, reward, next_state = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i] = current_state\n",
    "            targets[i] = model.predict(current_state)[0]\n",
    "            Q_sa = np.max(model.predict(next_state)[0])\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anves5\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch: 1 Score: -43.51399999999983 Epsilon: 0.99980\n",
      "Epoch: 2 Score: -38.860000000000035 Epsilon: 0.99960\n",
      "Epoch: 3 Score: -43.499999999999844 Epsilon: 0.99940\n",
      "Epoch: 4 Score: -40.37200000000006 Epsilon: 0.99920\n",
      "Epoch: 5 Score: -43.96499999999979 Epsilon: 0.99900\n",
      "Epoch: 6 Score: -39.364000000000054 Epsilon: 0.99880\n",
      "Epoch: 7 Score: -50.15100000000021 Epsilon: 0.99860\n",
      "Epoch: 8 Score: -49.584000000000245 Epsilon: 0.99840\n",
      "Epoch: 9 Score: -39.44800000000006 Epsilon: 0.99820\n",
      "Epoch: 10 Score: -39.61600000000006 Epsilon: 0.99800\n",
      "Epoch: 11 Score: -54.188000000000216 Epsilon: 0.99780\n",
      "Epoch: 12 Score: -39.02800000000004 Epsilon: 0.99760\n",
      "Epoch: 13 Score: -39.112000000000045 Epsilon: 0.99740\n",
      "Epoch: 14 Score: -39.112000000000045 Epsilon: 0.99720\n",
      "Epoch: 15 Score: -43.540999999999784 Epsilon: 0.99700\n",
      "Epoch: 16 Score: -43.87899999999979 Epsilon: 0.99680\n",
      "Epoch: 17 Score: -39.53200000000006 Epsilon: 0.99660\n",
      "Epoch: 18 Score: -42.660999999999845 Epsilon: 0.99640\n",
      "Epoch: 19 Score: -38.27200000000001 Epsilon: 0.99620\n",
      "Epoch: 20 Score: -42.298999999999815 Epsilon: 0.99600\n",
      "Epoch: 21 Score: -45.9249999999998 Epsilon: 0.99580\n",
      "Epoch: 22 Score: -39.02800000000004 Epsilon: 0.99560\n",
      "Epoch: 23 Score: -43.41699999999983 Epsilon: 0.99540\n",
      "Epoch: 24 Score: -49.61100000000026 Epsilon: 0.99520\n",
      "Epoch: 25 Score: -47.88099999999993 Epsilon: 0.99500\n",
      "Epoch: 26 Score: -39.53200000000006 Epsilon: 0.99480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-29608770609d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrentState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnextState\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgameOver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Sets the currenState to the nextState\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL-Learning-v3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Section where we will train our AI to play Breakout - Trainer\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# Defining the parameters (hyperparamteters)\n",
    "memSize = 50000              #The maximum size of the experience replay memory.\n",
    "batchSize = 10               #The size of the batch of inputs and targets gotten at each iteration from the experience replay memory for the model to train on.\n",
    "learningRate = 0.0001        #The learning rate for the Adam optimizer in the Brain.\n",
    "gamma = 0.9                  #The discount factor for the experience replay memory.\n",
    "nLastStates = 3              #How many last frames we save as our current state of the game. Remember, the input is a 3D array of size nRows x nColumns x nLastStates to the CNN in the Brain.\n",
    "epsilon = 1.                 #The initial epsilon, the chance of taking a random action.\n",
    "epsilonDecayRate = 0.0002    #By how much we decrease epsilon after every single game/epoch.\n",
    "minEpsilon = 0.05            #The lowest possible epsilon, after which it can't be adjusted any lower.\n",
    "filepathToSave = 'model2.h5' #Where we want to save the model.\n",
    "\n",
    "# Creating the Environment, the Brain and the Experience Replay Memory\n",
    "#env = environment(0)  --- maybe just remove this line\n",
    "brain = Brain()\n",
    "model = brain.model\n",
    "dqn = Dqn(memSize, gamma)\n",
    "\n",
    "# A function that will initialize game states\n",
    "def resetStates():\n",
    "    observation = rgb2gray(env.reset())\n",
    "    currentState = np.zeros((1, nRows, nColumns, nLastStates))\n",
    "    for i in range(nLastStates):\n",
    "        currentState[:,:,:,i] = observation\n",
    "    return currentState, currentState\n",
    "\n",
    "# A function that converts the picture to grayscaale\n",
    "def rgb2gray(rgb):\n",
    "    # Downsizing the image so the process of the convolution layers will be faster \n",
    "    downsized = cv2.resize(rgb, dsize=(nColumns, nRows), interpolation=cv2.INTER_CUBIC)\n",
    "    return np.dot(downsized[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "# Starting the main loop\n",
    "epoch = 0\n",
    "scores = list()\n",
    "maxScore = 0    #the highest score obtained so far in the training\n",
    "score = 0      #the score in each game/epoch\n",
    "\n",
    "while True:\n",
    "    # Resetting the environment and game states\n",
    "    currentState, nextState = resetStates()\n",
    "    epoch += 1\n",
    "    gameOver = False\n",
    "    previousLifes = 5\n",
    "    lifes = 5\n",
    "    livesStep = 0\n",
    "    score = -80\n",
    "    bricks = 84   # 14 bricks in a row and there are 6 rows in total, so a starting amount of 84 bricks\n",
    "        \n",
    "    # Starting the second loop in which we play the game and teach our AI\n",
    "    while not gameOver:\n",
    "        # Choosing an action to play\n",
    "        if np.random.rand() < epsilon:        #Checks if a random action sould be made, or just take the action with the highest Q-value.\n",
    "            action = np.random.randint(0, 3)\n",
    "        else:\n",
    "            qvalues = model.predict(currentState)[0]\n",
    "            action = np.argmax(qvalues)\n",
    "\n",
    "        # Updating the enviroenment\n",
    "        state, reward, gamOver, livesStep = env.step(action + 1)  # +1 is placed here as there are 4 actions, but the first action does nothing and hence it is ignored.\n",
    "        lifes = livesStep['ale.lives']        \n",
    "        # Render the game\n",
    "        env.render()\n",
    "        # converting the rgb image state to grayscale\n",
    "        state = rgb2gray(state)\n",
    "        \n",
    "\n",
    "        # Adding new game frame to the next state and deleting the oldest frame from next state\n",
    "        state = np.reshape(state, (1, nRows, nColumns, 1))\n",
    "        nextState = np.append(nextState, state, axis = 3)\n",
    "        nextState = np.delete(nextState, 0, axis = 3)\n",
    "        \n",
    "        # Remembering the transition and training the AI\n",
    "        dqn.remember([currentState, action, reward, nextState], gameOver)\n",
    "        inputs, targets = dqn.get_batch(model, batchSize)\n",
    "        model.train_on_batch(inputs, targets)\n",
    "        \n",
    "        # Sets the currenState to the nextState\n",
    "        currentState = nextState\n",
    "                \n",
    "        # Updates the score\n",
    "        score += reward\n",
    "        \n",
    "        # If a life was lost, reset environment\n",
    "        if lifes < previousLifes:\n",
    "            # Updating lifes\n",
    "            previousLifes -= 1\n",
    "            # granting it a negative reward for loosing a life\n",
    "            score -= 3\n",
    "        if lifes == 0:\n",
    "            gameOver = True\n",
    "    \n",
    "    # Checking if score record was beaten and if yes then saving the model\n",
    "    if score > maxScore:\n",
    "        maxScore = score\n",
    "        model.save(filepathToSave)\n",
    "    \n",
    "    # Lowering the epsilon\n",
    "    if epsilon > minEpsilon:\n",
    "        epsilon -= epsilonDecayRate\n",
    "    \n",
    "    # Showing the results each game\n",
    "    print('Epoch: ' + str(epoch) + ' Score: ' + str(score) + ' Epsilon: {:.5f}'.format(epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (RL-Learning)",
   "language": "python",
   "name": "rl-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
